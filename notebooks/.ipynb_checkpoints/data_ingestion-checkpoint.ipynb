{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens - Ingestion Batch et Vérification de la Qualité\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialisation de Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/29 23:29:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de Spark: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# Création d'une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLens Data Ingestion\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Version de Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## 2. Vérification HDFS préalable\n",
    "\n",
    "!hdfs dfs -ls /\n",
    "\n",
    "## 3. Création du répertoire de destination dans HDFS\n",
    "\n",
    "!hdfs dfs -mkdir -p /user/movielens/raw\n",
    "!hdfs dfs -ls /user/movielens\n",
    "\n",
    "## 4. Upload local → HDFS (depuis /notebooks/data ou autre)\n",
    "# Remplacer les chemins ci-dessous par le chemin réel dans le container Docker si différent\n",
    "\n",
    "!hdfs dfs -put -f /data/ratings.csv /user/movielens/raw/\n",
    "!hdfs dfs -put -f /data/movies.csv /user/movielens/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## 5. Vérification post-upload\n",
    "\n",
    "print(\"\\nContenu de /user/movielens/raw dans HDFS:\")\n",
    "!hdfs dfs -ls /user/movielens/raw\n",
    "\n",
    "## 6. Lecture avec Spark (vérification d'intégrité)\n",
    "\n",
    "ratings_path = \"hdfs://namenode:9000/user/movielens/raw/ratings.csv\"\n",
    "movies_path = \"hdfs://namenode:9000/user/movielens/raw/movies.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données locales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(ratings_path)\n",
    "movies_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(movies_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schéma des notes:\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Schéma des films:\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage des schémas\n",
    "print(\"Schéma des notes:\")\n",
    "ratings_df.printSchema()\n",
    "\n",
    "print(\"\\nSchéma des films:\")\n",
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des notes:\n",
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      2|   3.5|2005-04-02 23:53:47|\n",
      "|     1|     29|   3.5|2005-04-02 23:31:16|\n",
      "|     1|     32|   3.5|2005-04-02 23:33:39|\n",
      "|     1|     47|   3.5|2005-04-02 23:32:07|\n",
      "|     1|     50|   3.5|2005-04-02 23:29:40|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Aperçu des films:\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage des premières lignes\n",
    "print(\"Aperçu des notes:\")\n",
    "ratings_df.show(5)\n",
    "\n",
    "print(\"\\nAperçu des films:\")\n",
    "movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques sur les notes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+\n",
      "|summary|           userId|          movieId|            rating|\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "|  count|         20000263|         20000263|          20000263|\n",
      "|   mean|69045.87258292554|9041.567330339605|3.5255285642993797|\n",
      "| stddev| 40038.6266531621|  19789.477445413| 1.051988919294244|\n",
      "|    min|                1|                1|               0.5|\n",
      "|    max|           138493|           131262|               5.0|\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "\n",
      "\n",
      "Nombre total de notes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000263\n",
      "\n",
      "Nombre total de films:\n",
      "27278\n",
      "\n",
      "Nombre d'utilisateurs uniques:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                       (0 + 16) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Statistiques descriptives\n",
    "print(\"Statistiques sur les notes:\")\n",
    "ratings_df.describe().show()\n",
    "\n",
    "print(\"\\nNombre total de notes:\")\n",
    "print(ratings_df.count())\n",
    "\n",
    "print(\"\\nNombre total de films:\")\n",
    "print(movies_df.count())\n",
    "\n",
    "print(\"\\nNombre d'utilisateurs uniques:\")\n",
    "print(ratings_df.select(\"userId\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vérification de la qualité des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour compter les valeurs nulles\n",
    "def count_nulls(df):\n",
    "    null_counts = []\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype in [\"double\", \"float\"]:\n",
    "            condition = col(col_name).isNull() | isnan(col(col_name))\n",
    "        else:\n",
    "            condition = col(col_name).isNull()\n",
    "        null_counts.append(count(when(condition, col_name)).alias(col_name))\n",
    "    return df.select(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes dans les notes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     0|      0|     0|        0|\n",
      "+------+-------+------+---------+\n",
      "\n",
      "\n",
      "Valeurs manquantes dans les films:\n",
      "+-------+-----+------+\n",
      "|movieId|title|genres|\n",
      "+-------+-----+------+\n",
      "|      0|    0|     0|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vérification des valeurs manquantes\n",
    "print(\"Valeurs manquantes dans les notes:\")\n",
    "count_nulls(ratings_df).show()\n",
    "\n",
    "print(\"\\nValeurs manquantes dans les films:\")\n",
    "count_nulls(movies_df).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 23:29:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans ratings: 20000263\n",
      "Nombre de lignes distinctes dans ratings: 20000263\n",
      "Nombre de doublons dans ratings: 0\n",
      "Nombre de lignes dans movies: 27278\n",
      "Nombre de lignes distinctes dans movies: 27278\n",
      "Nombre de doublons dans movies: 0\n"
     ]
    }
   ],
   "source": [
    "# Vérification des doublons\n",
    "ratings_count = ratings_df.count()\n",
    "ratings_distinct_count = ratings_df.distinct().count()\n",
    "print(f\"Nombre de lignes dans ratings: {ratings_count}\")\n",
    "print(f\"Nombre de lignes distinctes dans ratings: {ratings_distinct_count}\")\n",
    "print(f\"Nombre de doublons dans ratings: {ratings_count - ratings_distinct_count}\")\n",
    "\n",
    "movies_count = movies_df.count()\n",
    "movies_distinct_count = movies_df.distinct().count()\n",
    "print(f\"Nombre de lignes dans movies: {movies_count}\")\n",
    "print(f\"Nombre de lignes distinctes dans movies: {movies_distinct_count}\")\n",
    "print(f\"Nombre de doublons dans movies: {movies_count - movies_distinct_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ingestion des données dans HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "Found 2 items\n",
      "drwxrwx---   - root supergroup          0 2025-04-29 14:38 /tmp\n",
      "drwxr-xr-x   - root supergroup          0 2025-04-29 23:24 /user\n"
     ]
    }
   ],
   "source": [
    "# Vérification que HDFS est en cours d'exécution\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "-m: Unknown command\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile [-n] <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum [-v] <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-concat <target path> <src path> <src path> ...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate] [-fs <path>]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des répertoires dans HDFS\n",
    "!hdfs dfs -mkdir -p /user/movielens/raw\n",
    "!hdfs dfs -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "put: `../data/movielens/ratings.csv': No such file or directory\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "put: `../data/movielens/movies.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Copie des fichiers CSV vers HDFS\n",
    "!hdfs dfs -put -f {ratings_file} /user/movielens/raw/\n",
    "!hdfs dfs -put -f {movies_file} /user/movielens/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    }
   ],
   "source": [
    "# Vérification des fichiers dans HDFS\n",
    "!hdfs dfs -ls /user/movielens/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vérification des données dans HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Lecture des données depuis HDFS\n",
    "ratings_hdfs = spark.read.csv(\"hdfs://namenode:9000/user/data/movielens/ratings.csv\", header=True, inferSchema=True)\n",
    "movies_hdfs = spark.read.csv(\"hdfs://namenode:9000/user/data/movielens/movies.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans ratings (HDFS): 20000263\n",
      "Nombre de lignes dans movies (HDFS): 27278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Vérification du nombre de lignes\n",
    "print(f\"Nombre de lignes dans ratings (HDFS): {ratings_hdfs.count()}\")\n",
    "print(f\"Nombre de lignes dans movies (HDFS): {movies_hdfs.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation de la distribution des notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Conversion en pandas pour la visualisation\n",
    "ratings_pandas = ratings_df.select(\"rating\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution des notes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ratings_pandas[\"rating\"], bins=10, kde=True)\n",
    "plt.title('Distribution des notes')\n",
    "plt.xlabel('Note')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Fermeture de la session Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
